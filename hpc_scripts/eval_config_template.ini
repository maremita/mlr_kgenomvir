######################################################################
####################### Update this sections #########################
######################################################################
#
[slurm]
account = def-account
mail_user = mail-user

[resources]
cpus_per_task = 1
# gpu:1 |Â gpu:v100l:1 | cpu
gres = gpu:1
mem = 60000M
time = 00:10:00
# /scratch
output_folder = ../data/output/hpc

[distr_evals]
#
k_list = 4, 9
eval_types = CC, CF, FF
penalties = none, l2, l1 
fragment_sizes = 250, 500
sim_iterations = 1,10
#
coverages = 0.1, 1, 2, 10
mutations = 0.0001, 0.001, 0.01, 0.1
indels = 0.0001, 0.001, 0.01, 0.1
recombs = 0.0001, 0.001, 0.01, 0.1
imbdata = 0, 5, 10, 50 
imbsamp = 0, 5, 10, 50
klens = 4, 5, 6, 7, 8, 9, 10
lambdas = 1e-3, 1e-1, 1.0, 1e+1, 1e+2
lrs = 1e-5, 1e-3, 1e-1
lowvars = 0.01, 0.1, 0.3, 0.5
nbclasses = 2, 5, 10, 20
# in case of recombs set dual infection prob to
rep_dual_infection = 0.01

######################################################################
########### The following will be updated automatically ##############
######################################################################
[job]
job_code = job

[io]
outdir = outdir

[simulation]
iterations = 10
# init_seq : [file, none]
# If none, a random nucleotide sequence will be generated
init_seq = none
init_seq_size = 5000
init_pop_size = 100
init_gen_count_fraction = 0.3
##
# Parameters related to class sizes
nb_classes = 10
class_pop_size = 50
class_pop_size_min = 5
class_pop_size_max = 100
# class_pop_size_std controls how
# simulated dataset is imbalanced
# class_pop_size will be used as mean
class_pop_size_std = 10
##
# Evolution related parameters
generation_count = 1000
fitness_freq = 0.5
rep_dual_infection = 0.0001
rep_recombination = 0.0001
mutation_rate = 0.0001
transition_bias = 2.0
# The 2 parameters of NB distr. for indel model
# seprated by a white space
indel_model_nb = 0.4 1
# Probability of insertion and deletion events
indel_prob = 0.0001

[seq_rep]
k = 4
full_kmers = False
low_var_threshold = 0.01
fragment_size = 500
fragment_count = 1
fragment_cov = 2
##
# Sampling dataset classes
# (before fragmentation if any)
class_size_min = 5
class_size_max = 100
class_size_mean = 50
# class_size_std controls the how
# sampling already-generated datasetis imbalanced
class_size_std = 0.

[evaluation] 
# CC, CF or FF 
eval_type = CC
test_size = 0.33
cv_folds = 5
# precision, recall, fscore or all
eval_metric = fscore
# micro, macro or weighted
avrg_metric = macro

[classifier]
module = pytorch_mlr
tol = 1e-5
lambda = 1
l1_ratio = 0.5
solver = sgd
max_iter = 1000
### for pytorch implementation
learning_rate = 0.0001
n_iter_no_change = 100
device = cuda
##
# these penalities will be evaluated
penalty = none, l2, l1

[settings]
n_main_jobs = 1
n_cv_jobs = ${evaluation:cv_folds}
verbose = 1
load_data = True
save_data = True
load_models = True
save_models = True
load_results = True
save_results = True
save_final_results = False
plot_results = False
random_state = 75
